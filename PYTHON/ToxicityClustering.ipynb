{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/andrew/DATA/git_repos/CAMP-RT/PYTHON/preprocessing.py:11: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from analysis import *\n",
    "from collections import namedtuple\n",
    "import Metrics\n",
    "from PatientSet import *\n",
    "from Constants import Constants\n",
    "from Clustering import *\n",
    "import re\n",
    "import copy\n",
    "\n",
    "#sklearn dependencies\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score, adjusted_rand_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "#we get like a million deprication errors for some reason with the external libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load in the patientset object that has all the patient info\n",
    "db = load_patientset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for the experiments\n",
    "toxicities_to_test = ['toxicity']\n",
    "\n",
    "#features to test the feature selection on.  should be fields in the patientset we have\n",
    "#we don't cluster on these\n",
    "unclusterable_features = ['t_volumes', 'bilateral', 'total_volumes']\n",
    "#we cluster on these (each individually) if feature_clustering is defined,\n",
    "clusterable_features =['tumor_distances', 'volumes','tsimdoses']\n",
    "\n",
    "#features specifically for feature selection vs actually using.  Should either be\n",
    "#some combo of actual and predicted dose for this\n",
    "train_features = [] #['doses']\n",
    "test_features = [] #['tsimdoses']\n",
    "\n",
    "#number of times to resample and doing feature selection\n",
    "#if n = 1, just use the first result\n",
    "n_samples = 500\n",
    "df_rescale = Metrics.normalize\n",
    "\n",
    "#for now just constrain it to one cluster\n",
    "n_clusters = 4\n",
    "mutual_info_thresh = 0.00001\n",
    "\n",
    "#number of cpus to use when running the feature selection\n",
    "n_jobs = 8\n",
    "\n",
    "#make this none if you don't want to cluster the features\n",
    "#otherwise give a number of clusters to group them, \n",
    "#should be <33 as of writting this as that is the total included number of organs\n",
    "feature_clustering = None\n",
    "\n",
    "#where to save results, put None if you don't want to save\n",
    "save_root = 'data/clustering_results/'\n",
    "run_file_name = 'metaClusteringBootstrapped500NormalizedBest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity\n",
      "['Mandible_tsimdoses', 'Soft_Palate_tsimdoses']\n",
      "4.358258332115561e-06\n",
      "\n",
      "['Mandible_tsimdoses', 'Soft_Palate_tsimdoses', 'Supraglottic_Larynx_tumor_distances']\n",
      "1.1195306930389393e-06\n",
      "\n",
      "\n",
      "\n",
      "[[ 28.  25.]\n",
      " [138.   9.]]\n",
      "2.2512500903036762e-10\n",
      "number of features:  3\n",
      "Kmeans4\n",
      "[[46.  3.]\n",
      " [22. 10.]\n",
      " [85.  4.]\n",
      " [13. 17.]]\n",
      "correlation:  2.0204774835444274e-10\n",
      "rand score:  0.13549667031488893 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#our actual experiment, try to find correlations using the boruta method and such\n",
    "feature_list = []\n",
    "for tox_name in toxicities_to_test:\n",
    "    print(tox_name)\n",
    "    toxicity = getattr(db, tox_name) > 0\n",
    "    \n",
    "    #remove eyeball stuff from the candidates since those are missing in some patients\n",
    "    #and it messe100s up the feature selection due to that noise\n",
    "    organs = copy.copy(Constants.organ_list)\n",
    "    for o in Constants.organ_list:\n",
    "        if re.search('Eyeball', o) is not None:\n",
    "            organs.remove(o)\n",
    "    \n",
    "    train, _ = get_train_test_datasets(db, unclusterable_features, \n",
    "                                          clusterable_features, \n",
    "                                          train_features, \n",
    "                                          test_features,\n",
    "                                         organs,\n",
    "                                         feature_clustering)\n",
    "    \n",
    "    if df_rescale is not None:\n",
    "        train = df_rescale(train)\n",
    "        \n",
    "    if mutual_info_thresh >= 0:\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        info_scores = mutual_info_classif(train.values, toxicity)\n",
    "        good_args = np.argwhere(info_scores > mutual_info_thresh).ravel()\n",
    "        train = train.iloc[:,good_args]\n",
    "    \n",
    "    selection_clusterer = BestClusterer(max_clusters = 5)\n",
    "#     selection_clusterer  = FClusterer(n_clusters, dist_func = l2)\n",
    "    feature_selector = FeatureClusterSelector(\n",
    "        n_samples = n_samples,\n",
    "        model = selection_clusterer,\n",
    "        n_jobs = n_jobs).fit(train, toxicity)\n",
    "    to_use = feature_selector.transform(train)\n",
    "    labels = feature_selector.predict_labels(train, toxicity)\n",
    "    print()\n",
    "    print()\n",
    "    print(get_contingency_table(labels, toxicity))\n",
    "    print(fisher_exact_test(labels, toxicity))\n",
    "    print('number of features: ', to_use.shape[1])\n",
    "\n",
    "    #we're going to try a bunch of different clusterings and look at the best result\n",
    "    clustering = get_optimal_clustering(to_use.values, \n",
    "                                        toxicity,\n",
    "                                       min_clusters = n_clusters,\n",
    "                                       max_clusters = n_clusters)\n",
    "    print(clustering[1].method)\n",
    "    print(get_contingency_table(clustering[0], toxicity))\n",
    "    print('correlation: ', clustering[1].correlation)\n",
    "    print('rand score: ', clustering[1].rand_score,'\\n')\n",
    "\n",
    "    to_use['cluster_labels'] = clustering[0]\n",
    "#     print(best_features.columns)\n",
    "    to_use.index.rename('Dummy.ID', inplace = True)\n",
    "    feature_list.append(to_use)\n",
    "    if save_root is not None:\n",
    "        n_best_clusters = len(set(clustering[0]))\n",
    "        to_use.to_csv(save_root\n",
    "                     + 'boruta_features_k='\n",
    "                     + str(n_best_clusters)\n",
    "                     + '_p=' + '{:.3e}'.format(clustering[1].correlation)\n",
    "                     + '_toxicity=' + tox_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['cluster_labels'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1bc7ac22ce97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Dummy.ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cluster_labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m combined_clusters = get_optimal_clustering(combined_df.values, db.toxicity,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                                            \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                                            \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                                            errors=errors)\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m     @rewrite_axis_style_signature('mapper', [('copy', True),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3780\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3810\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4964\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4965\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4966\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4967\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['cluster_labels'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#in the case where I try out testing vs multiple outcomes (i.e. feeding tubes alone and then aspiration)\n",
    "#this will combine all the features chosen for each run intoa single set\n",
    "#otherwise it should basically serve as a sanity check for the clustering results\n",
    "combined_df = feature_list[0]\n",
    "if len(feature_list) > 1:\n",
    "    for i in range(1, len(feature_list)):\n",
    "        df2 = feature_list[i]\n",
    "        to_drop = list(set(combined_df.columns).intersection(set(df2.columns)))\n",
    "        if len(to_drop) == df2.shape[1]:\n",
    "            continue\n",
    "        df2 = df2.drop(to_drop, axis = 1)\n",
    "        combined_df = pd.merge(combined_df, df2, on = 'Dummy.ID')\n",
    "combined_df.drop('cluster_labels', axis = 1, inplace = True)\n",
    "print(combined_df.columns)\n",
    "combined_clusters = get_optimal_clustering(combined_df.values, db.toxicity,\n",
    "                                    min_clusters = n_clusters,\n",
    "                                    max_clusters = n_clusters)\n",
    "print(combined_clusters[1].method)\n",
    "print(get_contingency_table(combined_clusters[0], toxicity))\n",
    "print('correlation: ', combined_clusters[1].correlation)\n",
    "print('rand score: ', combined_clusters[1].rand_score, '\\n')\n",
    "combined_df['cluster_labels'] = combined_clusters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the reults\n",
    "if save_root is not None:\n",
    "    combined_df.to_csv(save_root\n",
    "                 + run_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  0.1650500289545297 %\n",
      "using augmented similarities\n",
      "successfully save patient data to  ../data/patient_dataset.json\n",
      "successfully saved similarity score matrix to  data/scores.csv\n"
     ]
    }
   ],
   "source": [
    "export(db, method = 'tsim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
